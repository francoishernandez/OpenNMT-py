{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use OpenNMT-py as a Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example notebook (available [here](https://github.com/OpenNMT/OpenNMT-py/blob/master/docs/source/Library.ipynb)) should be able to run as a standalone execution, provided `onmt` is in the path (installed via `pip` for instance).\n",
    "\n",
    "Some parts may not be 100% 'library-friendly' but it's mostly workable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import a few modules and functions that will be necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from argparse import Namespace\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onmt\n",
    "from onmt.inputters.inputter import _load_vocab, _build_fields_vocab, get_fields, IterOnDevice\n",
    "from onmt.dynamic.corpus import ParallelCorpus\n",
    "from onmt.dynamic.iterator import DynamicDatasetIter\n",
    "from onmt.translate import GNMTGlobalScorer, Translator, TranslationBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RootLogger root (INFO)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# enable logging\n",
    "from onmt.utils.logging import init_logger, logger\n",
    "init_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a proper example, we will need some data, as well as some vocabulary(ies).\n",
    "\n",
    "Let's take the same data as in the [quickstart](https://opennmt.net/OpenNMT-py/quickstart.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-09-22 16:31:08--  https://s3.amazonaws.com/opennmt-trainingdata/toy-ende.tar.gz\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.97.181\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.97.181|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1662081 (1,6M) [application/x-gzip]\n",
      "Saving to: ‘toy-ende.tar.gz.15’\n",
      "\n",
      "toy-ende.tar.gz.15  100%[===================>]   1,58M  2,17MB/s    in 0,7s    \n",
      "\n",
      "2020-09-22 16:31:09 (2,17 MB/s) - ‘toy-ende.tar.gz.15’ saved [1662081/1662081]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3.amazonaws.com/opennmt-trainingdata/toy-ende.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar xf toy-ende.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.yaml  src-test.txt   src-val.txt   tgt-train.txt\r\n",
      "\u001b[0m\u001b[01;34mrun\u001b[0m/         src-train.txt  tgt-test.txt  tgt-val.txt\r\n"
     ]
    }
   ],
   "source": [
    "ls toy-ende"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data and vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for any use case of OpenNMT-py 2.0, we can start by creating a simple YAML configuration with our datasets. This is the easiest way to build the proper `opts` `Namespace` that will be used to create the vocabulary(ies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_config = \"\"\"\n",
    "## Where the vocab(s) will be written\n",
    "save_data: toy-ende/run/example\n",
    "# Corpus opts:\n",
    "data:\n",
    "    corpus:\n",
    "        path_src: toy-ende/src-train.txt\n",
    "        path_tgt: toy-ende/tgt-train.txt\n",
    "        transforms: []\n",
    "        weight: 1\n",
    "    valid:\n",
    "        path_src: toy-ende/src-val.txt\n",
    "        path_tgt: toy-ende/tgt-val.txt\n",
    "        transforms: []\n",
    "\"\"\"\n",
    "config = yaml.safe_load(yaml_config)\n",
    "with open(\"toy-ende/config.yaml\", \"w\") as f:\n",
    "    f.write(yaml_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onmt.dynamic.parse import DynamicArgumentParser\n",
    "parser = DynamicArgumentParser(description='build_vocab.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onmt.dynamic.opts import dynamic_prepare_opts\n",
    "dynamic_prepare_opts(parser, build_vocab_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_args = ([\"-config\", \"toy-ende/config.yaml\", \"-n_sample\", \"10000\"])\n",
    "opts, unknown = parser.parse_known_args(base_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(config='toy-ende/config.yaml', data=\"{'corpus': {'path_src': 'toy-ende/src-train.txt', 'path_tgt': 'toy-ende/tgt-train.txt', 'transforms': [], 'weight': 1}, 'valid': {'path_src': 'toy-ende/src-val.txt', 'path_tgt': 'toy-ende/tgt-val.txt', 'transforms': []}}\", insert_ratio=0.0, mask_length='subword', mask_ratio=0.0, n_sample=10000, onmttok_kwargs=\"{'mode': 'none'}\", overwrite=False, permute_sent_ratio=0.0, poisson_lambda=0.0, random_ratio=0.0, replace_length=-1, rotate_ratio=0.5, save_config=None, save_data='toy-ende/run/example', seed=-1, share_vocab=False, src_seq_length=200, src_subword_model=None, src_subword_type='none', src_vocab=None, subword_alpha=0, subword_nbest=1, switchout_temperature=1.0, tgt_seq_length=200, tgt_subword_model=None, tgt_subword_type='none', tgt_vocab=None, tokendrop_temperature=1.0, tokenmask_temperature=1.0, transforms=[])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-09-22 16:31:09,862 WARNING] Corpus valid's weight should be given. We default it to 1 for you.\n",
      "[2020-09-22 16:31:09,866 INFO] Parsed 2 corpora from -data.\n",
      "[2020-09-22 16:31:09,868 INFO] Counter vocab from 10000 samples.\n",
      "[2020-09-22 16:31:09,871 INFO] corpus's transforms: TransformPipe()\n",
      "[2020-09-22 16:31:09,901 INFO] Loading ParallelCorpus(toy-ende/src-train.txt, toy-ende/tgt-train.txt, align=None)...\n",
      "[2020-09-22 16:31:10,084 INFO] Loading ParallelCorpus(toy-ende/src-train.txt, toy-ende/tgt-train.txt, align=None)...\n",
      "[2020-09-22 16:31:10,115 INFO] Counters src:24995\n",
      "[2020-09-22 16:31:10,116 INFO] Counters tgt:35816\n"
     ]
    }
   ],
   "source": [
    "from onmt.bin.build_vocab import build_vocab_main\n",
    "build_vocab_main(opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example.vocab.src  example.vocab.tgt  \u001b[0m\u001b[01;34msample\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls toy-ende/run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just created our source and target vocabularies, respectively `toy-ende/run/example.vocab.src` and `toy-ende/run/example.vocab.tgt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can build the fields from the text files that were just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_path = \"toy-ende/run/example.vocab.src\"\n",
    "tgt_vocab_path = \"toy-ende/run/example.vocab.tgt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-09-22 16:31:11,523 INFO] Loading src vocabulary from toy-ende/run/example.vocab.src\n",
      "[2020-09-22 16:31:11,584 INFO] Loaded src vocab has 24995 tokens.\n",
      "[2020-09-22 16:31:11,593 INFO] Loading tgt vocabulary from toy-ende/run/example.vocab.tgt\n",
      "[2020-09-22 16:31:11,647 INFO] Loaded tgt vocab has 35816 tokens.\n"
     ]
    }
   ],
   "source": [
    "# initialize the frequency counter\n",
    "counters = defaultdict(Counter)\n",
    "# load source vocab\n",
    "_src_vocab, _src_vocab_size = _load_vocab(\n",
    "    src_vocab_path,\n",
    "    'src',\n",
    "    counters)\n",
    "# load target vocab\n",
    "_tgt_vocab, _tgt_vocab_size = _load_vocab(\n",
    "    tgt_vocab_path,\n",
    "    'tgt',\n",
    "    counters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize fields\n",
    "src_nfeats, tgt_nfeats = 0, 0 # do not support word features for now\n",
    "fields = get_fields(\n",
    "    'text', src_nfeats, tgt_nfeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src': <onmt.inputters.text_dataset.TextMultiField at 0x7fb613e8b748>,\n",
       " 'tgt': <onmt.inputters.text_dataset.TextMultiField at 0x7fb613e8b828>,\n",
       " 'indices': <torchtext.data.field.Field at 0x7fb613e8bf60>}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-09-22 16:31:12,362 INFO]  * tgt vocab size: 30004.\n",
      "[2020-09-22 16:31:12,412 INFO]  * src vocab size: 24997.\n"
     ]
    }
   ],
   "source": [
    "# build fields vocab\n",
    "share_vocab = False\n",
    "vocab_size_multiple = 1\n",
    "src_vocab_size = 30000\n",
    "tgt_vocab_size = 30000\n",
    "src_words_min_frequency = 1\n",
    "tgt_words_min_frequency = 1\n",
    "vocab_fields = _build_fields_vocab(\n",
    "    fields, counters, 'text', share_vocab,\n",
    "    vocab_size_multiple,\n",
    "    src_vocab_size, src_words_min_frequency,\n",
    "    tgt_vocab_size, tgt_words_min_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative way of creating these fields is to run `onmt_train` without actually training, to just output the necessary files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare for training: model and optimizer creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a few fields/vocab related variables to simplify the model creation a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text_field = vocab_fields[\"src\"].base_field\n",
    "src_vocab = src_text_field.vocab\n",
    "src_padding = src_vocab.stoi[src_text_field.pad_token]\n",
    "\n",
    "tgt_text_field = vocab_fields['tgt'].base_field\n",
    "tgt_vocab = tgt_text_field.vocab\n",
    "tgt_padding = tgt_vocab.stoi[tgt_text_field.pad_token]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we specify the core model itself. Here we will build a small model with an encoder and an attention based input feeding decoder. Both models will be RNNs and the encoder will be bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 100\n",
    "rnn_size = 500\n",
    "# Specify the core model.\n",
    "\n",
    "encoder_embeddings = onmt.modules.Embeddings(emb_size, len(src_vocab),\n",
    "                                             word_padding_idx=src_padding)\n",
    "\n",
    "encoder = onmt.encoders.RNNEncoder(hidden_size=rnn_size, num_layers=1,\n",
    "                                   rnn_type=\"LSTM\", bidirectional=True,\n",
    "                                   embeddings=encoder_embeddings)\n",
    "\n",
    "decoder_embeddings = onmt.modules.Embeddings(emb_size, len(tgt_vocab),\n",
    "                                             word_padding_idx=tgt_padding)\n",
    "decoder = onmt.decoders.decoder.InputFeedRNNDecoder(\n",
    "    hidden_size=rnn_size, num_layers=1, bidirectional_encoder=True, \n",
    "    rnn_type=\"LSTM\", embeddings=decoder_embeddings)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = onmt.models.model.NMTModel(encoder, decoder)\n",
    "model.to(device)\n",
    "\n",
    "# Specify the tgt word generator and loss computation module\n",
    "model.generator = nn.Sequential(\n",
    "    nn.Linear(rnn_size, len(tgt_vocab)),\n",
    "    nn.LogSoftmax(dim=-1)).to(device)\n",
    "\n",
    "loss = onmt.utils.loss.NMTLossCompute(\n",
    "    criterion=nn.NLLLoss(ignore_index=tgt_padding, reduction=\"sum\"),\n",
    "    generator=model.generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set up the optimizer. This could be a core torch optim class, or our wrapper which handles learning rate updates and gradient normalization automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1\n",
    "torch_optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "optim = onmt.utils.optimizers.Optimizer(\n",
    "    torch_optimizer, learning_rate=lr, max_grad_norm=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the training and validation data iterators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create the dynamic dataset iterator.\n",
    "\n",
    "This is not very 'library-friendly' for now because of the way the `DynamicDatasetIter` constructor is defined. It may evolve in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_train = \"toy-ende/src-train.txt\"\n",
    "tgt_train = \"toy-ende/tgt-train.txt\"\n",
    "src_val = \"toy-ende/src-val.txt\"\n",
    "tgt_val = \"toy-ende/tgt-val.txt\"\n",
    "\n",
    "# build the ParallelCorpus\n",
    "corpus = ParallelCorpus(src_train, tgt_train)\n",
    "valid = ParallelCorpus(src_val, tgt_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora = {\"corpus\": corpus}\n",
    "transforms = {}\n",
    "opts = Namespace()\n",
    "opts.batch_size = 4096\n",
    "opts.batch_type = \"tokens\"\n",
    "opts.valid_batch_size = 8\n",
    "opts.batch_size_multiple = 1\n",
    "opts.data_type = \"text\"\n",
    "opts.bucket_size = 4096\n",
    "opts.pool_factor = 100\n",
    "opts.data = {\"corpus\": {\"weight\": 1}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the training iterator\n",
    "is_train = True\n",
    "train_iter = DynamicDatasetIter(\n",
    "    corpora, transforms, vocab_fields, opts, is_train,\n",
    "    stride=1, offset=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the iteration happens on GPU 0 (-1 for CPU, N for GPU N)\n",
    "train_iter = iter(IterOnDevice(train_iter, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora = {\"valid\": valid}\n",
    "transforms = {}\n",
    "opts = Namespace()\n",
    "opts.batch_size = 4096\n",
    "opts.batch_type = \"tokens\"\n",
    "opts.valid_batch_size = 8\n",
    "opts.batch_size_multiple = 1\n",
    "opts.data_type = \"text\"\n",
    "opts.bucket_size = 4096\n",
    "opts.pool_factor = 100\n",
    "opts.data = {\"valid\": {\"weight\": 1}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the validation iterator\n",
    "is_train = False\n",
    "valid_iter = DynamicDatasetIter(\n",
    "    corpora, transforms, vocab_fields, opts, is_train,\n",
    "    stride=1, offset=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_iter = IterOnDevice(valid_iter, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-09-22 16:31:18,569 INFO] Start training loop and validate every 500 steps...\n",
      "[2020-09-22 16:31:18,572 INFO] corpus's transforms: TransformPipe()\n",
      "[2020-09-22 16:31:18,574 INFO] Loading ParallelCorpus(toy-ende/src-train.txt, toy-ende/tgt-train.txt, align=None)...\n",
      "[2020-09-22 16:31:24,717 INFO] Step 50/ 1000; acc:   7.21; ppl: 6659.24; xent: 8.80; lr: 1.00000; 19307/19183 tok/s;      6 sec\n",
      "[2020-09-22 16:31:28,225 INFO] Loading ParallelCorpus(toy-ende/src-train.txt, toy-ende/tgt-train.txt, align=None)...\n",
      "[2020-09-22 16:31:30,779 INFO] Step 100/ 1000; acc:   9.26; ppl: 1989.38; xent: 7.60; lr: 1.00000; 19281/19312 tok/s;     12 sec\n",
      "[2020-09-22 16:31:36,770 INFO] Step 150/ 1000; acc:  10.16; ppl: 1374.93; xent: 7.23; lr: 1.00000; 18835/18653 tok/s;     18 sec\n",
      "[2020-09-22 16:31:37,902 INFO] Loading ParallelCorpus(toy-ende/src-train.txt, toy-ende/tgt-train.txt, align=None)...\n",
      "[2020-09-22 16:31:42,944 INFO] Step 200/ 1000; acc:  11.11; ppl: 1114.67; xent: 7.02; lr: 1.00000; 19001/18907 tok/s;     24 sec\n",
      "[2020-09-22 16:31:49,075 INFO] Step 250/ 1000; acc:  12.05; ppl: 940.74; xent: 6.85; lr: 1.00000; 19266/19120 tok/s;     31 sec\n",
      "[2020-09-22 16:31:52,315 INFO] Loading ParallelCorpus(toy-ende/src-train.txt, toy-ende/tgt-train.txt, align=None)...\n",
      "[2020-09-22 16:31:55,233 INFO] Step 300/ 1000; acc:  13.13; ppl: 756.69; xent: 6.63; lr: 1.00000; 18918/18918 tok/s;     37 sec\n",
      "[2020-09-22 16:32:01,301 INFO] Step 350/ 1000; acc:  13.84; ppl: 673.48; xent: 6.51; lr: 1.00000; 18444/18335 tok/s;     43 sec\n",
      "[2020-09-22 16:32:02,179 INFO] Loading ParallelCorpus(toy-ende/src-train.txt, toy-ende/tgt-train.txt, align=None)...\n",
      "[2020-09-22 16:32:07,458 INFO] Step 400/ 1000; acc:  14.74; ppl: 579.51; xent: 6.36; lr: 1.00000; 19241/19129 tok/s;     49 sec\n",
      "[2020-09-22 16:32:13,644 INFO] Step 450/ 1000; acc:  16.07; ppl: 507.73; xent: 6.23; lr: 1.00000; 18889/18905 tok/s;     55 sec\n",
      "[2020-09-22 16:32:16,765 INFO] Loading ParallelCorpus(toy-ende/src-train.txt, toy-ende/tgt-train.txt, align=None)...\n",
      "[2020-09-22 16:32:19,841 INFO] Step 500/ 1000; acc:  16.48; ppl: 456.66; xent: 6.12; lr: 1.00000; 19076/18925 tok/s;     61 sec\n",
      "[2020-09-22 16:32:19,842 INFO] valid's transforms: TransformPipe()\n",
      "[2020-09-22 16:32:19,844 INFO] Loading ParallelCorpus(toy-ende/src-val.txt, toy-ende/tgt-val.txt, align=None)...\n",
      "[2020-09-22 16:32:28,779 INFO] Validation perplexity: 276.766\n",
      "[2020-09-22 16:32:28,780 INFO] Validation accuracy: 20.0439\n",
      "[2020-09-22 16:32:34,824 INFO] Step 550/ 1000; acc:  17.48; ppl: 404.68; xent: 6.00; lr: 1.00000; 7507/7418 tok/s;     76 sec\n",
      "[2020-09-22 16:32:35,565 INFO] Loading ParallelCorpus(toy-ende/src-train.txt, toy-ende/tgt-train.txt, align=None)...\n",
      "[2020-09-22 16:32:41,112 INFO] Step 600/ 1000; acc:  18.90; ppl: 348.96; xent: 5.85; lr: 1.00000; 18067/18046 tok/s;     83 sec\n",
      "[2020-09-22 16:32:47,363 INFO] Step 650/ 1000; acc:  19.69; ppl: 311.28; xent: 5.74; lr: 1.00000; 18561/18595 tok/s;     89 sec\n",
      "[2020-09-22 16:32:50,417 INFO] Loading ParallelCorpus(toy-ende/src-train.txt, toy-ende/tgt-train.txt, align=None)...\n",
      "[2020-09-22 16:32:53,650 INFO] Step 700/ 1000; acc:  20.55; ppl: 279.91; xent: 5.63; lr: 1.00000; 18735/18615 tok/s;     95 sec\n",
      "[2020-09-22 16:32:59,905 INFO] Step 750/ 1000; acc:  21.90; ppl: 243.30; xent: 5.49; lr: 1.00000; 18580/18489 tok/s;    101 sec\n",
      "[2020-09-22 16:33:00,493 INFO] Loading ParallelCorpus(toy-ende/src-train.txt, toy-ende/tgt-train.txt, align=None)...\n",
      "[2020-09-22 16:33:06,243 INFO] Step 800/ 1000; acc:  23.01; ppl: 215.40; xent: 5.37; lr: 1.00000; 17972/17859 tok/s;    108 sec\n",
      "[2020-09-22 16:33:10,432 INFO] Loading ParallelCorpus(toy-ende/src-train.txt, toy-ende/tgt-train.txt, align=None)...\n",
      "[2020-09-22 16:33:12,454 INFO] Step 850/ 1000; acc:  24.39; ppl: 188.56; xent: 5.24; lr: 1.00000; 18979/18915 tok/s;    114 sec\n",
      "[2020-09-22 16:33:18,704 INFO] Step 900/ 1000; acc:  25.39; ppl: 169.35; xent: 5.13; lr: 1.00000; 18709/18341 tok/s;    120 sec\n",
      "[2020-09-22 16:33:24,976 INFO] Step 950/ 1000; acc:  26.46; ppl: 150.88; xent: 5.02; lr: 1.00000; 18393/18571 tok/s;    126 sec\n",
      "[2020-09-22 16:33:25,388 INFO] Loading ParallelCorpus(toy-ende/src-train.txt, toy-ende/tgt-train.txt, align=None)...\n",
      "[2020-09-22 16:33:31,180 INFO] Step 1000/ 1000; acc:  27.87; ppl: 132.36; xent: 4.89; lr: 1.00000; 18655/18517 tok/s;    133 sec\n",
      "[2020-09-22 16:33:31,182 INFO] Loading ParallelCorpus(toy-ende/src-val.txt, toy-ende/tgt-val.txt, align=None)...\n",
      "[2020-09-22 16:33:40,079 INFO] Validation perplexity: 218.311\n",
      "[2020-09-22 16:33:40,080 INFO] Validation accuracy: 21.7181\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<onmt.utils.statistics.Statistics at 0x7fb613a1ba90>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_manager = onmt.utils.ReportMgr(\n",
    "    report_every=50, start_time=None, tensorboard_writer=None)\n",
    "\n",
    "trainer = onmt.Trainer(model=model,\n",
    "                       train_loss=loss,\n",
    "                       valid_loss=loss,\n",
    "                       optim=optim,\n",
    "                       report_manager=report_manager,\n",
    "                       dropout=[0.1])\n",
    "\n",
    "trainer.train(train_iter=train_iter,\n",
    "              train_steps=1000,\n",
    "              valid_iter=valid_iter,\n",
    "              valid_steps=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For translation, we can build a \"traditional\" (as opposed to dynamic) dataset for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_data = {\"reader\": onmt.inputters.str2reader[\"text\"](), \"data\": src_val}\n",
    "tgt_data = {\"reader\": onmt.inputters.str2reader[\"text\"](), \"data\": tgt_val}\n",
    "_readers, _data = onmt.inputters.Dataset.config(\n",
    "    [('src', src_data), ('tgt', tgt_data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = onmt.inputters.Dataset(\n",
    "    vocab_fields, readers=_readers, data=_data,\n",
    "    sort_key=onmt.inputters.str2sortkey[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = onmt.inputters.OrderedIterator(\n",
    "            dataset=dataset,\n",
    "            device=\"cuda\",\n",
    "            batch_size=10,\n",
    "            train=False,\n",
    "            sort=False,\n",
    "            sort_within_batch=True,\n",
    "            shuffle=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_reader = onmt.inputters.str2reader[\"text\"]\n",
    "tgt_reader = onmt.inputters.str2reader[\"text\"]\n",
    "scorer = GNMTGlobalScorer(alpha=0.7, \n",
    "                          beta=0., \n",
    "                          length_penalty=\"avg\", \n",
    "                          coverage_penalty=\"none\")\n",
    "gpu = 0 if torch.cuda.is_available() else -1\n",
    "translator = Translator(model=model, \n",
    "                        fields=vocab_fields, \n",
    "                        src_reader=src_reader, \n",
    "                        tgt_reader=tgt_reader, \n",
    "                        global_scorer=scorer,\n",
    "                        gpu=gpu)\n",
    "builder = onmt.translate.TranslationBuilder(data=dataset, \n",
    "                                            fields=vocab_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: translations will be very poor, because of the very low quantity of data, the absence of proper tokenization, and the brevity of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SENT 0: ['Parliament', 'Does', 'Not', 'Support', 'Amendment', 'Freeing', 'Tymoshenko']\n",
      "PRED 0: Das Parlament , die Europäische Parlament , die dem Parlament , dem Parlament , dem Parlament , dem Parlament , dem Parlament zu <unk> .\n",
      "PRED SCORE: -1.6008\n",
      "\n",
      "\n",
      "SENT 0: ['Today', ',', 'the', 'Ukraine', 'parliament', 'dismissed', ',', 'within', 'the', 'Code', 'of', 'Criminal', 'Procedure', 'amendment', ',', 'the', 'motion', 'to', 'revoke', 'an', 'article', 'based', 'on', 'which', 'the', 'opposition', 'leader', ',', 'Yulia', 'Tymoshenko', ',', 'was', 'sentenced', '.']\n",
      "PRED 0: Die Aussprache , die sich in den letzten Jahren , die in der Lage , die in der Lage , die in der Lage , die in der Lage , die in der Lage , die in der Lage , die in der Lage , die in der Lage , die in der Lage , die in der Lage , die in der Lage , die in der Lage , die in der Lage , die in der Lage , die in der Lage , die in der Lage , die in der Lage , die in der Lage ,\n",
      "PRED SCORE: -1.7200\n",
      "\n",
      "\n",
      "SENT 0: ['The', 'amendment', 'that', 'would', 'lead', 'to', 'freeing', 'the', 'imprisoned', 'former', 'Prime', 'Minister', 'was', 'revoked', 'during', 'second', 'reading', 'of', 'the', 'proposal', 'for', 'mitigation', 'of', 'sentences', 'for', 'economic', 'offences', '.']\n",
      "PRED 0: Die Tatsache , die im Rahmen war , war es , die in der Lage , die im Vorschlag , war .\n",
      "PRED SCORE: -1.6019\n",
      "\n",
      "\n",
      "SENT 0: ['In', 'October', ',', 'Tymoshenko', 'was', 'sentenced', 'to', 'seven', 'years', 'in', 'prison', 'for', 'entering', 'into', 'what', 'was', 'reported', 'to', 'be', 'a', 'disadvantageous', 'gas', 'deal', 'with', 'Russia', '.']\n",
      "PRED 0: In einigen Jahren war es , um zu einem Jahren zu <unk> , was die Zusammenarbeit mit Russland .\n",
      "PRED SCORE: -1.5860\n",
      "\n",
      "\n",
      "SENT 0: ['The', 'verdict', 'is', 'not', 'yet', 'final;', 'the', 'court', 'will', 'hear', 'Tymoshenko', '&apos;s', 'appeal', 'in', 'December', '.']\n",
      "PRED 0: Die Aussprache ist nicht <unk> , die <unk> <unk> und <unk> <unk> .\n",
      "PRED SCORE: -1.4980\n",
      "\n",
      "\n",
      "SENT 0: ['Tymoshenko', 'claims', 'the', 'verdict', 'is', 'a', 'political', 'revenge', 'of', 'the', 'regime;', 'in', 'the', 'West', ',', 'the', 'trial', 'has', 'also', 'evoked', 'suspicion', 'of', 'being', 'biased', '.']\n",
      "PRED 0: Es ist eine Frage der <unk> , in der Nähe , die in der Lage , die sich in der Lage , die in der Lage , in denen auch <unk> .\n",
      "PRED SCORE: -1.8194\n",
      "\n",
      "\n",
      "SENT 0: ['The', 'proposal', 'to', 'remove', 'Article', '365', 'from', 'the', 'Code', 'of', 'Criminal', 'Procedure', ',', 'upon', 'which', 'the', 'former', 'Prime', 'Minister', 'was', 'sentenced', ',', 'was', 'supported', 'by', '147', 'members', 'of', 'parliament', '.']\n",
      "PRED 0: Der Vorschlag , die sich von den Menschen , die in der Türkei , die in der Lage , die in der Lage , die in der Lage , die in der Lage , die in der Lage , die in der Lage , die in der Lage , die in der Lage , die in der Lage , die in der Lage , die in der Lage , die in der Lage , die in der Lage , die in der Lage , die in der Lage , die in der Lage , die in der Lage , die\n",
      "PRED SCORE: -1.6393\n",
      "\n",
      "\n",
      "SENT 0: ['Its', 'ratification', 'would', 'require', '226', 'votes', '.']\n",
      "PRED 0: Bitte beachten Sie , dass sie <unk> werden .\n",
      "PRED SCORE: -1.4621\n",
      "\n",
      "\n",
      "SENT 0: ['Libya', '&apos;s', 'Victory']\n",
      "PRED 0: In der Nähe ist es , wenn man eine <unk> , <unk> , <unk> .\n",
      "PRED SCORE: -1.9260\n",
      "\n",
      "\n",
      "SENT 0: ['The', 'story', 'of', 'Libya', '&apos;s', 'liberation', ',', 'or', 'rebellion', ',', 'already', 'has', 'its', 'defeated', '.']\n",
      "PRED 0: Die Firma , <unk> oder <unk> , <unk> , <unk> , <unk> , <unk> .\n",
      "PRED SCORE: -1.6742\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for batch in data_iter:\n",
    "    trans_batch = translator.translate_batch(\n",
    "        batch=batch, src_vocabs=[src_vocab],\n",
    "        attn_debug=False)\n",
    "    translations = builder.from_batch(trans_batch)\n",
    "    for trans in translations:\n",
    "        print(trans.log(0))\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
