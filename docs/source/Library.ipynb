{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onmt\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from onmt.inputters.inputter import _load_vocab, _build_fields_vocab, get_fields, IterOnDevice\n",
    "from onmt.dynamic.corpus import ParallelCorpus\n",
    "from onmt.dynamic.iterator import DynamicDatasetIter\n",
    "from argparse import Namespace\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RootLogger root (INFO)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# enable logging\n",
    "from onmt.utils.logging import init_logger, logger\n",
    "init_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-09-22 12:55:36--  https://s3.amazonaws.com/opennmt-trainingdata/toy-ende.tar.gz\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.88.237\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.88.237|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1662081 (1,6M) [application/x-gzip]\n",
      "Saving to: ‘toy-ende.tar.gz’\n",
      "\n",
      "toy-ende.tar.gz     100%[===================>]   1,58M  2,55MB/s    in 0,6s    \n",
      "\n",
      "2020-09-22 12:55:37 (2,55 MB/s) - ‘toy-ende.tar.gz’ saved [1662081/1662081]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3.amazonaws.com/opennmt-trainingdata/toy-ende.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar xf toy-ende.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src-test.txt   src-val.txt   tgt-train.txt\r\n",
      "src-train.txt  tgt-test.txt  tgt-val.txt\r\n"
     ]
    }
   ],
   "source": [
    "ls toy-ende"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data and vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_config = \"\"\"\n",
    "## Where the vocab(s) will be written\n",
    "save_data: toy-ende/run/example\n",
    "# Corpus opts:\n",
    "data:\n",
    "    corpus:\n",
    "        path_src: toy-ende/src-train.txt\n",
    "        path_tgt: toy-ende/tgt-train.txt\n",
    "        transforms: []\n",
    "        weight: 1\n",
    "    valid:\n",
    "        path_src: data/src-val.txt\n",
    "        path_tgt: data/tgt-val.txt\n",
    "        transforms: []\n",
    "\"\"\"\n",
    "config = yaml.safe_load(yaml_config)\n",
    "with open(\"toy-ende/config.yaml\", \"w\") as f:\n",
    "    f.write(yaml_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onmt.dynamic.parse import DynamicArgumentParser\n",
    "parser = DynamicArgumentParser(description='build_vocab.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onmt.dynamic.opts import dynamic_prepare_opts\n",
    "dynamic_prepare_opts(parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Namespace(config='toy-ende/config.yaml', data=\"{'corpus': {'path_src': 'toy-ende/src-train.txt', 'path_tgt': 'toy-ende/tgt-train.txt', 'transforms': [], 'weight': 1}, 'valid': {'path_src': 'data/src-val.txt', 'path_tgt': 'data/tgt-val.txt', 'transforms': []}}\", dynamic_dict=False, fast_align_model=None, fast_align_root=None, insert_ratio=0.0, lemmatize_target=50, mask_length='subword', mask_ratio=0.0, max_num_tags=3, max_tag_id=10, max_term_tokens=3, max_terms=3, n_sample=-1, onmttok_kwargs=\"{'mode': 'none'}\", overlap='nested', overwrite=False, permute_sent_ratio=0.0, poisson_lambda=0.0, random_ratio=0.0, replace_length=-1, rotate_ratio=0.5, save_config=None, save_data='toy-ende/run/example', share_vocab=False, src_seq_length=200, src_seq_length_trunc=None, src_subword_model=None, src_subword_type='none', src_vocab='toto', src_vocab_size=50000, src_words_min_frequency=0, subword_alpha=0, subword_nbest=1, switchout_temperature=1.0, tags_ratio=0.1, terminology_ratio=0.1, tgt_seq_length=200, tgt_seq_length_trunc=None, tgt_subword_model=None, tgt_subword_type='none', tgt_vocab=None, tgt_vocab_size=50000, tgt_words_min_frequency=0, tokendrop_temperature=1.0, tokenmask_temperature=1.0, transforms=[], vocab_size_multiple=1),\n",
       " [])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_args = ([\"-config\", \"toy-ende/config.yaml\", \"-src_vocab\", \"toto\"])\n",
    "parser.parse_known_args(base_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to build the vocab from the text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_path = \"../../toy-ende/run/example.vocab.src\"\n",
    "tgt_vocab_path = \"../../toy-ende/run/example.vocab.tgt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the frequency counter\n",
    "counters = defaultdict(Counter)\n",
    "# load source vocab\n",
    "_src_vocab, _src_vocab_size = _load_vocab(\n",
    "    src_vocab_path,\n",
    "    'src',\n",
    "    counters,\n",
    "    min_freq=1)\n",
    "# load target vocab\n",
    "_tgt_vocab, _tgt_vocab_size = _load_vocab(\n",
    "    tgt_vocab_path,\n",
    "    'tgt',\n",
    "    counters,\n",
    "    min_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize fields\n",
    "src_nfeats, tgt_nfeats = 0, 0 # do not support word features for now\n",
    "fields = get_fields(\n",
    "    'text', src_nfeats, tgt_nfeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src': <onmt.inputters.text_dataset.TextMultiField at 0x7fb9cf548cc0>,\n",
       " 'tgt': <onmt.inputters.text_dataset.TextMultiField at 0x7fb9cebe4898>,\n",
       " 'indices': <torchtext.data.field.Field at 0x7fb9cebe4978>}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build fields vocab\n",
    "share_vocab = False\n",
    "vocab_size_multiple = 1\n",
    "src_vocab_size = 30000\n",
    "tgt_vocab_size = 30000\n",
    "src_words_min_frequency = 1\n",
    "tgt_words_min_frequency = 1\n",
    "vocab_fields = _build_fields_vocab(\n",
    "    fields, counters, 'text', share_vocab,\n",
    "    vocab_size_multiple,\n",
    "    src_vocab_size, src_words_min_frequency,\n",
    "    tgt_vocab_size, tgt_words_min_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative way of creating these fields is to run `onmt_train` without actually training, to just output the necessary files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text_field = vocab_fields[\"src\"].base_field\n",
    "src_vocab = src_text_field.vocab\n",
    "src_padding = src_vocab.stoi[src_text_field.pad_token]\n",
    "\n",
    "tgt_text_field = vocab_fields['tgt'].base_field\n",
    "tgt_vocab = tgt_text_field.vocab\n",
    "tgt_padding = tgt_vocab.stoi[tgt_text_field.pad_token]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we specify the core model itself. Here we will build a small model with an encoder and an attention based input feeding decoder. Both models will be RNNs and the encoder will be bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 100\n",
    "rnn_size = 500\n",
    "# Specify the core model.\n",
    "\n",
    "encoder_embeddings = onmt.modules.Embeddings(emb_size, len(src_vocab),\n",
    "                                             word_padding_idx=src_padding)\n",
    "\n",
    "encoder = onmt.encoders.RNNEncoder(hidden_size=rnn_size, num_layers=1,\n",
    "                                   rnn_type=\"LSTM\", bidirectional=True,\n",
    "                                   embeddings=encoder_embeddings)\n",
    "\n",
    "decoder_embeddings = onmt.modules.Embeddings(emb_size, len(tgt_vocab),\n",
    "                                             word_padding_idx=tgt_padding)\n",
    "decoder = onmt.decoders.decoder.InputFeedRNNDecoder(\n",
    "    hidden_size=rnn_size, num_layers=1, bidirectional_encoder=True, \n",
    "    rnn_type=\"LSTM\", embeddings=decoder_embeddings)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = onmt.models.model.NMTModel(encoder, decoder)\n",
    "model.to(device)\n",
    "\n",
    "# Specify the tgt word generator and loss computation module\n",
    "model.generator = nn.Sequential(\n",
    "    nn.Linear(rnn_size, len(tgt_vocab)),\n",
    "    nn.LogSoftmax(dim=-1)).to(device)\n",
    "\n",
    "loss = onmt.utils.loss.NMTLossCompute(\n",
    "    criterion=nn.NLLLoss(ignore_index=tgt_padding, reduction=\"sum\"),\n",
    "    generator=model.generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set up the optimizer. This could be a core torch optim class, or our wrapper which handles learning rate updates and gradient normalization automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1\n",
    "torch_optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "optim = onmt.utils.optimizers.Optimizer(\n",
    "    torch_optimizer, learning_rate=lr, max_grad_norm=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create the dynamic dataset iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_train = \"../../toy-ende/src-train.txt\"\n",
    "tgt_train = \"../../toy-ende/tgt-train.txt\"\n",
    "src_val = \"../../toy-ende/src-val.txt\"\n",
    "tgt_val = \"../../toy-ende/tgt-val.txt\"\n",
    "\n",
    "# build the ParallelCorpus\n",
    "corpus = ParallelCorpus(src_train, tgt_train)\n",
    "valid = ParallelCorpus(src_val, tgt_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora = {\"corpus\": corpus}\n",
    "transforms = {}\n",
    "opts = Namespace()\n",
    "opts.batch_size = 4096\n",
    "opts.batch_type = \"tokens\"\n",
    "opts.valid_batch_size = 8\n",
    "opts.batch_size_multiple = 1\n",
    "opts.data_type = \"text\"\n",
    "opts.bucket_size = 4096\n",
    "opts.pool_factor = 100\n",
    "opts.data = {\"corpus\": {\"weight\": 1}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### corpora_info {'corpus': {'weight': 1}}\n"
     ]
    }
   ],
   "source": [
    "# build the dataset iterator\n",
    "is_train = True\n",
    "train_iter = DynamicDatasetIter(\n",
    "    corpora, transforms, vocab_fields, opts, is_train,\n",
    "    stride=1, offset=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(IterOnDevice(train_iter, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora = {\"valid\": valid}\n",
    "transforms = {}\n",
    "opts = Namespace()\n",
    "opts.batch_size = 4096\n",
    "opts.batch_type = \"tokens\"\n",
    "opts.valid_batch_size = 8\n",
    "opts.batch_size_multiple = 1\n",
    "opts.data_type = \"text\"\n",
    "opts.bucket_size = 4096\n",
    "opts.pool_factor = 100\n",
    "opts.data = {\"valid\": {\"weight\": 1}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### corpora_info {'valid': {'weight': 1}}\n"
     ]
    }
   ],
   "source": [
    "# build the dataset iterator\n",
    "is_train = False\n",
    "valid_iter = DynamicDatasetIter(\n",
    "    corpora, transforms, vocab_fields, opts, is_train,\n",
    "    stride=1, offset=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_iter = IterOnDevice(valid_iter, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-09-22 12:52:42,213 INFO] Start training loop and validate every 200 steps...\n",
      "[2020-09-22 12:52:42,214 INFO] corpus's transforms: TransformPipe()\n",
      "[2020-09-22 12:52:42,215 INFO] Loading ParallelCorpus(../../toy-ende/src-train.txt, ../../toy-ende/tgt-train.txt, align=None)...\n",
      "[2020-09-22 12:52:48,315 INFO] Step 50/  400; acc:   7.77; ppl: 4517.68; xent: 8.42; lr: 1.00000; 19806/19563 tok/s;      6 sec\n",
      "[2020-09-22 12:52:51,717 INFO] Loading ParallelCorpus(../../toy-ende/src-train.txt, ../../toy-ende/tgt-train.txt, align=None)...\n",
      "[2020-09-22 12:52:54,251 INFO] Step 100/  400; acc:   9.42; ppl: 1884.87; xent: 7.54; lr: 1.00000; 18663/18757 tok/s;     12 sec\n",
      "[2020-09-22 12:53:00,160 INFO] Step 150/  400; acc:   9.94; ppl: 1438.19; xent: 7.27; lr: 1.00000; 19698/19626 tok/s;     18 sec\n",
      "[2020-09-22 12:53:01,289 INFO] Loading ParallelCorpus(../../toy-ende/src-train.txt, ../../toy-ende/tgt-train.txt, align=None)...\n",
      "[2020-09-22 12:53:06,249 INFO] Step 200/  400; acc:  11.20; ppl: 1121.74; xent: 7.02; lr: 1.00000; 19404/19209 tok/s;     24 sec\n",
      "[2020-09-22 12:53:06,251 INFO] valid's transforms: TransformPipe()\n",
      "[2020-09-22 12:53:06,252 INFO] Loading ParallelCorpus(../../toy-ende/src-val.txt, ../../toy-ende/tgt-val.txt, align=None)...\n",
      "[2020-09-22 12:53:14,991 INFO] {'loss': 503973.33740234375, 'n_words': 74666, 'n_correct': 6297, 'n_src_words': 0, 'start_time': 1600771986.2513683}\n",
      "[2020-09-22 12:53:14,992 INFO] Validation perplexity: 853.805\n",
      "[2020-09-22 12:53:14,992 INFO] Validation accuracy: 8.43356\n",
      "[2020-09-22 12:53:21,058 INFO] Step 250/  400; acc:  12.35; ppl: 901.21; xent: 6.80; lr: 1.00000; 8008/7906 tok/s;     39 sec\n",
      "[2020-09-22 12:53:24,189 INFO] Loading ParallelCorpus(../../toy-ende/src-train.txt, ../../toy-ende/tgt-train.txt, align=None)...\n",
      "[2020-09-22 12:53:26,992 INFO] Step 300/  400; acc:  13.50; ppl: 773.76; xent: 6.65; lr: 1.00000; 19100/19269 tok/s;     45 sec\n",
      "[2020-09-22 12:53:32,964 INFO] Step 350/  400; acc:  14.18; ppl: 686.00; xent: 6.53; lr: 1.00000; 19146/19036 tok/s;     51 sec\n",
      "[2020-09-22 12:53:33,893 INFO] Loading ParallelCorpus(../../toy-ende/src-train.txt, ../../toy-ende/tgt-train.txt, align=None)...\n",
      "[2020-09-22 12:53:39,105 INFO] Step 400/  400; acc:  14.82; ppl: 590.28; xent: 6.38; lr: 1.00000; 19453/19268 tok/s;     57 sec\n",
      "[2020-09-22 12:53:39,107 INFO] Loading ParallelCorpus(../../toy-ende/src-val.txt, ../../toy-ende/tgt-val.txt, align=None)...\n",
      "[2020-09-22 12:53:47,765 INFO] {'loss': 443286.89767456055, 'n_words': 74666, 'n_correct': 10099, 'n_src_words': 0, 'start_time': 1600772019.10675}\n",
      "[2020-09-22 12:53:47,766 INFO] Validation perplexity: 378.771\n",
      "[2020-09-22 12:53:47,766 INFO] Validation accuracy: 13.5256\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<onmt.utils.statistics.Statistics at 0x7fb9c0241588>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "report_manager = onmt.utils.ReportMgr(\n",
    "    report_every=50, start_time=None, tensorboard_writer=None)\n",
    "\n",
    "trainer = onmt.Trainer(model=model,\n",
    "                       train_loss=loss,\n",
    "                       valid_loss=loss,\n",
    "                       optim=optim,\n",
    "                       report_manager=report_manager)\n",
    "\n",
    "trainer.train(train_iter=train_iter,\n",
    "              train_steps=400,\n",
    "              valid_iter=valid_iter,\n",
    "              valid_steps=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the model, we need to load up the translation functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import onmt.translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED SCORE: -4.0690\n",
      "\n",
      "SENT 0: ('The', 'competitors', 'have', 'other', 'advantages', ',', 'too', '.')\n",
      "PRED 0: .\n",
      "\n",
      "PRED SCORE: -4.2736\n",
      "\n",
      "SENT 0: ('The', 'company', '&apos;s', 'durability', 'goes', 'back', 'to', 'its', 'first', 'boss', ',', 'a', 'visionary', ',', 'Thomas', 'J.', 'Watson', 'Sr.')\n",
      "PRED 0: .\n",
      "\n",
      "PRED SCORE: -4.0144\n",
      "\n",
      "SENT 0: ('&quot;', 'From', 'what', 'we', 'know', 'today', ',', 'you', 'have', 'to', 'ask', 'how', 'I', 'could', 'be', 'so', 'wrong', '.', '&quot;')\n",
      "PRED 0: .\n",
      "\n",
      "PRED SCORE: -4.1361\n",
      "\n",
      "SENT 0: ('Boeing', 'Co', 'shares', 'rose', '1.5%', 'to', '$', '67.94', '.')\n",
      "PRED 0: .\n",
      "\n",
      "PRED SCORE: -4.1382\n",
      "\n",
      "SENT 0: ('Some', 'did', 'not', 'believe', 'him', ',', 'they', 'said', 'that', 'he', 'got', 'dizzy', 'even', 'in', 'the', 'truck', ',', 'but', 'always', 'wanted', 'to', 'fulfill', 'his', 'dream', ',', 'that', 'of', 'becoming', 'a', 'pilot', '.')\n",
      "PRED 0: .\n",
      "\n",
      "PRED SCORE: -3.8881\n",
      "\n",
      "SENT 0: ('In', 'your', 'opinion', ',', 'the', 'council', 'should', 'ensure', 'that', 'the', 'band', 'immediately', 'above', 'the', 'Ronda', 'de', 'Dalt', 'should', 'provide', 'in', 'its', 'entirety', ',', 'an', 'area', 'of', 'equipment', 'to', 'conduct', 'a', 'smooth', 'transition', 'between', 'the', 'city', 'and', 'the', 'green', '.')\n",
      "PRED 0: .\n",
      "\n",
      "PRED SCORE: -4.0778\n",
      "\n",
      "SENT 0: ('The', 'clerk', 'of', 'the', 'court', ',', 'Jorge', 'Yanez', ',', 'went', 'to', 'the', 'jail', 'of', 'the', 'municipality', 'of', 'San', 'Nicolas', 'of', 'Garza', 'to', 'notify', 'Jonah', 'that', 'he', 'has', 'been', 'legally', 'pardoned', 'and', 'his', 'record', 'will', 'be', 'filed', '.')\n",
      "PRED 0: .\n",
      "\n",
      "PRED SCORE: -4.2479\n",
      "\n",
      "SENT 0: ('&quot;', 'In', 'a', 'research', 'it', 'is', 'reported', 'that', 'there', 'are', 'no', 'parts', 'or', 'components', 'of', 'the', 'ship', 'in', 'another', 'place', ',', 'the', 'impact', 'is', 'presented', 'in', 'a', 'structural', 'way', '.')\n",
      "PRED 0: .\n",
      "\n",
      "PRED SCORE: -3.8585\n",
      "\n",
      "SENT 0: ('On', 'the', 'asphalt', 'covering', ',', 'he', 'added', ',', 'is', 'placed', 'a', 'final', 'layer', 'called', 'rolling', 'covering', ',', 'which', 'is', 'made', '\\u200b', '\\u200b', 'of', 'a', 'fine', 'stone', 'material', ',', 'meaning', 'sand', 'also', 'dipped', 'into', 'the', 'asphalt', '.')\n",
      "PRED 0: .\n",
      "\n",
      "PRED SCORE: -4.2298\n",
      "\n",
      "SENT 0: ('This', 'is', '200', 'bar', 'on', 'leaving', 'and', '100', 'bar', 'on', 'arrival', '.')\n",
      "PRED 0: .\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/torch/tensor.py:297: UserWarning: other is not broadcastable to self, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n",
      "  return self.add_(other)\n"
     ]
    }
   ],
   "source": [
    "translator = onmt.translate.Translator(beam_size=10, fields=data.fields, model=model)\n",
    "builder = onmt.translate.TranslationBuilder(data=valid_data, fields=data.fields)\n",
    "\n",
    "valid_data.src_vocabs\n",
    "for batch in valid_iter:\n",
    "    trans_batch = translator.translate_batch(batch=batch, data=valid_data)\n",
    "    translations = builder.from_batch(trans_batch)\n",
    "    for trans in translations:\n",
    "        print(trans.log(0))\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
